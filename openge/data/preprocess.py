"""Data preprocessing utilities for normalization, imputation, and feature engineering."""

import numpy as np
import pandas as pd
from typing import Union, Tuple, Optional, List, Dict
from sklearn.preprocessing import StandardScaler, MinMaxScaler


# ä½œç‰©ç”Ÿè‚²æœŸå®šä¹‰
CROP_GROWTH_STAGES = {
    'maize': {
        'VE': (0, 10),
        'V_stage': (10, 40),
        'VT': (40, 60),
        'R_stage': (60, 120),
    },
    'wheat': {
        'Emergence': (0, 15),
        'Tillering': (15, 45),
        'Booting': (45, 60),
        'Flowering': (60, 75),
        'Grain_filling': (75, 120),
    },
    'rice': {
        'Emergence': (0, 15),
        'Tillering': (15, 60),
        'Booting': (60, 80),
        'Flowering': (80, 100),
        'Maturity': (100, 150),
    }
}


class Preprocessor:
    """Universal preprocessor for genetic and environmental data."""
    
    def __init__(self, method: str = "standard"):
        """
        Initialize preprocessor.
        
        Args:
            method: Normalization method ('standard' or 'minmax')
        """
        self.method = method
        self.scaler = StandardScaler() if method == "standard" else MinMaxScaler()
        self.is_fitted = False
        
    def normalize(self, data: np.ndarray, fit: bool = True) -> np.ndarray:
        """
        Normalize data.
        
        Args:
            data: Input data array (2D)
            fit: Whether to fit the scaler (True for training, False for inference)
            
        Returns:
            Normalized data
        """
        if data.ndim != 2:
            raise ValueError(f"æœŸæœ› 2D æ•°ç»„ï¼Œä½†å¾—åˆ° {data.ndim}D")
        
        if fit:
            normalized = self.scaler.fit_transform(data)
            self.is_fitted = True
        else:
            if not self.is_fitted:
                raise ValueError("Scaler æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨ normalize(data, fit=True)")
            normalized = self.scaler.transform(data)
        
        return normalized.astype(np.float32)
    
    def inverse_normalize(self, data: np.ndarray) -> np.ndarray:
        """
        åå½’ä¸€åŒ–æ•°æ®
        
        Args:
            data: å½’ä¸€åŒ–åçš„æ•°æ®
            
        Returns:
            åŸå§‹å°ºåº¦çš„æ•°æ®
        """
        if not self.is_fitted:
            raise ValueError("Scaler æœªæ‹Ÿåˆ")
        return self.scaler.inverse_transform(data).astype(np.float32)
    
    def handle_missing_values(self, data: np.ndarray, strategy: str = "mean") -> np.ndarray:
        """
        Handle missing values in NumPy array.
        
        Args:
            data: Input data with potential NaN values (2D)
            strategy: Imputation strategy ('mean', 'median', 'zero', 'forward_fill')
            
        Returns:
            Data with missing values filled
        """
        if data.ndim != 2:
            raise ValueError(f"æœŸæœ› 2D æ•°ç»„ï¼Œä½†å¾—åˆ° {data.ndim}D")
        
        result = data.copy()
        
        if strategy == 'mean':
            col_means = np.nanmean(result, axis=0)
            for j in range(result.shape[1]):
                mask = np.isnan(result[:, j])
                result[mask, j] = col_means[j]
        
        elif strategy == 'median':
            col_medians = np.nanmedian(result, axis=0)
            for j in range(result.shape[1]):
                mask = np.isnan(result[:, j])
                result[mask, j] = col_medians[j]
        
        elif strategy == 'zero':
            result = np.nan_to_num(result, nan=0.0)
        
        elif strategy == 'forward_fill':
            for j in range(result.shape[1]):
                for i in range(1, result.shape[0]):
                    if np.isnan(result[i, j]):
                        result[i, j] = result[i-1, j]
        
        else:
            raise ValueError(f"æœªçŸ¥çš„å¡«å……ç­–ç•¥: {strategy}")
        
        return result.astype(np.float32)
    
    def feature_engineering(self, genetic_data: np.ndarray, env_data: np.ndarray) -> np.ndarray:
        """
        Create interaction features between genetic and environmental data.
        
        Args:
            genetic_data: Genetic data array (n_samples, n_genetic_features)
            env_data: Environmental data array (n_samples, n_env_features)
            
        Returns:
            Feature matrix with GÃ—E interactions (n_samples, n_genetic * n_env)
        """
        if genetic_data.shape[0] != env_data.shape[0]:
            raise ValueError(f"æ ·æœ¬æ•°ä¸åŒ¹é…: genetic={genetic_data.shape[0]}, env={env_data.shape[0]}")
        
        n_samples = genetic_data.shape[0]
        n_genetic = genetic_data.shape[1]
        n_env = env_data.shape[1]
        
        # åˆ›å»º GÃ—E äº¤äº’ç‰¹å¾
        interactions = np.zeros((n_samples, n_genetic * n_env), dtype=np.float32)
        
        for i in range(n_genetic):
            for j in range(n_env):
                interactions[:, i * n_env + j] = genetic_data[:, i] * env_data[:, j]
        
        return interactions


def check_and_handle_missing(df: pd.DataFrame, 
                             method: str = 'drop',
                             threshold: float = 0.5,
                             name: str = 'Data') -> pd.DataFrame:
    """
    æ£€æŸ¥å’Œå¤„ç† DataFrame ä¸­çš„ç¼ºå¤±å€¼
    
    Parameters:
        df: è¾“å…¥ DataFrame
        method: å¤„ç†æ–¹æ³• ('drop', 'forward_fill', 'backward_fill', 'mean', 'interpolate', 'none')
        threshold: ç¼ºå¤±ç‡é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼çš„åˆ—å°†è¢«åˆ é™¤
        name: æ•°æ®åç§°ï¼ˆç”¨äºæ‰“å°ä¿¡æ¯ï¼‰
    
    Returns:
        å¤„ç†åçš„ DataFrame
    """
    n_rows, n_cols = df.shape
    n_missing_total = df.isna().sum().sum()
    
    if n_missing_total == 0:
        print(f"âœ“ {name}: æ— ç¼ºå¤±å€¼")
        return df
    
    print(f"\nğŸ“Š {name} ç¼ºå¤±å€¼åˆ†æ:")
    print(f"   æ€»ç¼ºå¤±æ•°ï¼š{n_missing_total} / {n_rows * n_cols} ({100 * n_missing_total / (n_rows * n_cols):.2f}%)")
    
    missing_per_col = df.isna().sum()
    cols_with_missing = missing_per_col[missing_per_col > 0]
    
    print(f"   æœ‰ç¼ºå¤±å€¼çš„åˆ—ï¼š{len(cols_with_missing)}")
    for col, count in cols_with_missing.items():
        missing_rate = 100 * count / n_rows
        print(f"      â€¢ {col}: {count} ({missing_rate:.1f}%)")
    
    missing_per_row = df.isna().sum(axis=1)
    rows_with_missing = (missing_per_row > 0).sum()
    print(f"   æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼š{rows_with_missing}")
    
    # åˆ é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„åˆ—
    high_missing_cols = missing_per_col[missing_per_col / n_rows > threshold]
    if len(high_missing_cols) > 0:
        print(f"\n   ğŸ—‘ï¸ åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡ {100*threshold:.0f}% çš„åˆ—ï¼š")
        for col in high_missing_cols.index:
            print(f"      â€¢ {col} ({100 * high_missing_cols[col] / n_rows:.1f}%)")
        df = df.drop(columns=high_missing_cols.index)
    
    if method == 'none':
        print(f"\n   â­ï¸ è·³è¿‡ç¼ºå¤±å€¼å¤„ç†")
        return df
    
    elif method == 'drop':
        n_before = len(df)
        df = df.dropna()
        n_after = len(df)
        print(f"\n   ğŸ—‘ï¸ åˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼š{n_before - n_after} è¡Œè¢«åˆ é™¤")
        
    elif method == 'forward_fill':
        df = df.fillna(method='ffill')
        print(f"   âœ“ ä½¿ç”¨å‘å‰å¡«å……å¤„ç†ç¼ºå¤±å€¼")
        
    elif method == 'backward_fill':
        df = df.fillna(method='bfill')
        print(f"   âœ“ ä½¿ç”¨å‘åå¡«å……å¤„ç†ç¼ºå¤±å€¼")
        
    elif method == 'mean':
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].isna().sum() > 0:
                mean_val = df[col].mean()
                df[col].fillna(mean_val, inplace=True)
        print(f"   âœ“ ä½¿ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼")
        
    elif method == 'interpolate':
        df = df.interpolate(method='linear', limit_direction='both')
        print(f"   âœ“ ä½¿ç”¨æ’å€¼å¡«å……ç¼ºå¤±å€¼")
        
    else:
        raise ValueError(f"æœªçŸ¥çš„å¤„ç†æ–¹æ³•: {method}")
    
    n_missing_after = df.isna().sum().sum()
    if n_missing_after == 0:
        print(f"   âœ… å¤„ç†å®Œæˆï¼šæ— å‰©ä½™ç¼ºå¤±å€¼")
    else:
        print(f"   âš ï¸ å¤„ç†åä»æœ‰ {n_missing_after} ä¸ªç¼ºå¤±å€¼")
    
    return df


def aggregate_temporal_to_static(data: np.ndarray) -> np.ndarray:
    """
    å°† 3D æ—¶é—´åºåˆ—æ•°æ®èšåˆä¸º 2D é™æ€æ•°æ®
    
    Parameters:
        data: 3D æ•°ç»„ï¼Œå½¢çŠ¶ (n_samples, n_timesteps, n_features)
    
    Returns:
        2D æ•°ç»„ï¼Œå½¢çŠ¶ (n_samples, n_features * 4)
        æ¯ä¸ªåŸå§‹ç‰¹å¾äº§ç”Ÿ 4 ä¸ªèšåˆç‰¹å¾ï¼šmean, max, min, std
    """
    if data.ndim != 3:
        raise ValueError(f"æœŸæœ› 3D æ•°ç»„ï¼Œä½†å¾—åˆ° {data.ndim}Dï¼Œå½¢çŠ¶: {data.shape}")
    
    n_samples, n_timesteps, n_features = data.shape
    
    n_nan = np.isnan(data).sum()
    if n_nan > 0:
        nan_rate = 100 * n_nan / (n_samples * n_timesteps * n_features)
        print(f"âš ï¸ æ³¨æ„ï¼šèšåˆæ•°æ®ä¸­æœ‰ {n_nan} ä¸ª NaN ({nan_rate:.2f}%)")
    
    mean_val = np.nanmean(data, axis=1)
    max_val = np.nanmax(data, axis=1)
    min_val = np.nanmin(data, axis=1)
    std_val = np.nanstd(data, axis=1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ä¸º NaN çš„æ ·æœ¬
    invalid_samples = np.where(np.isnan(mean_val).any(axis=1))[0]
    if len(invalid_samples) > 0:
        print(f"âŒ é”™è¯¯ï¼šå‘ç° {len(invalid_samples)} ä¸ªå®Œå…¨ä¸º NaN çš„æ ·æœ¬")
        raise ValueError("å­˜åœ¨å®Œå…¨ä¸º NaN çš„æ ·æœ¬ï¼Œæ— æ³•èšåˆ")
    
    aggregated = np.concatenate([mean_val, max_val, min_val, std_val], axis=1)
    return aggregated.astype(np.float32)


def aggregate_temporal_features(data: np.ndarray, 
                               temporal_windows: Union[list, str, dict] = None,
                               crop_name: str = 'maize',
                               return_feature_names: bool = False) -> Union[np.ndarray, Tuple]:
    """
    æŒ‰æ—¶é—´çª—å£èšåˆæ—¶é—´ç‰¹å¾
    
    Parameters:
        data: 3D æ•°ç»„ï¼Œå½¢çŠ¶ (n_samples, n_timesteps, n_features)
        temporal_windows: æ—¶é—´çª—å£å®šä¹‰
            - str: ä½œç‰©åç§° ('maize', 'wheat', 'rice')ï¼Œä½¿ç”¨é¢„å®šä¹‰ç”Ÿè‚²æœŸ
            - list: çª—å£å¤§å°åˆ—è¡¨ [30, 60, 90]
            - dict: è‡ªå®šä¹‰çª—å£ {'stage1': (0, 30), 'stage2': (30, 60)}
        crop_name: ä½œç‰©åç§°ï¼ˆå½“ temporal_windows ä¸º str æ—¶ä½¿ç”¨ï¼‰
        return_feature_names: æ˜¯å¦è¿”å›ç‰¹å¾ååˆ—è¡¨
    
    Returns:
        å¦‚æœ return_feature_names=False:
            np.ndarray: èšåˆç‰¹å¾ (n_samples, n_aggregated_features)
        å¦‚æœ return_feature_names=True:
            Tuple[np.ndarray, List[str]]: (èšåˆç‰¹å¾, ç‰¹å¾ååˆ—è¡¨)
    """
    if data.ndim != 3:
        raise ValueError(f"æœŸæœ› 3D æ•°ç»„ï¼Œä½†å¾—åˆ° {data.ndim}Dï¼Œå½¢çŠ¶: {data.shape}")
    
    n_samples, n_timesteps, n_features = data.shape
    aggregated_list = []
    
    # ç¡®å®šæ—¶é—´çª—å£
    if isinstance(temporal_windows, str):
        window_info = get_growth_stages(temporal_windows)
    elif isinstance(temporal_windows, dict):
        window_info = temporal_windows
    elif isinstance(temporal_windows, list):
        window_info = create_fixed_windows(temporal_windows, n_timesteps)
    else:
        raise ValueError("temporal_windows å¿…é¡»æ˜¯ listã€str æˆ– dict")
    
    # æŒ‰çª—å£èšåˆ
    for stage_name, (start_idx, end_idx) in window_info.items():
        original_start, original_end = start_idx, end_idx
        start_idx = max(0, start_idx)
        end_idx = min(n_timesteps, end_idx)
        
        if start_idx >= end_idx:
            continue
        
        window_data = data[:, start_idx:end_idx, :]
        
        mean_val = np.nanmean(window_data, axis=1)
        max_val = np.nanmax(window_data, axis=1)
        min_val = np.nanmin(window_data, axis=1)
        std_val = np.nanstd(window_data, axis=1)
        
        aggregated_list.append((f"{stage_name}_mean", mean_val))
        aggregated_list.append((f"{stage_name}_max", max_val))
        aggregated_list.append((f"{stage_name}_min", min_val))
        aggregated_list.append((f"{stage_name}_std", std_val))
    
    if not aggregated_list:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´çª—å£ï¼")
    
    feature_names = [name for name, _ in aggregated_list]
    feature_arrays = [arr for _, arr in aggregated_list]
    aggregated_features = np.concatenate(feature_arrays, axis=1)
    
    if return_feature_names:
        return aggregated_features.astype(np.float32), feature_names
    
    return aggregated_features.astype(np.float32)


def get_growth_stages(crop_name: str) -> Dict[str, tuple]:
    """
    è·å–ä½œç‰©ç”Ÿè‚²æœŸå®šä¹‰
    
    Parameters:
        crop_name: ä½œç‰©åç§° ('maize', 'wheat', 'rice')
    
    Returns:
        Dict: ç”Ÿè‚²æœŸåç§°åˆ° (å¼€å§‹æ—¥, ç»“æŸæ—¥) çš„æ˜ å°„
    """
    crop_name = crop_name.lower()
    if crop_name not in CROP_GROWTH_STAGES:
        raise ValueError(f"ä¸æ”¯æŒçš„ä½œç‰©: {crop_name}ã€‚æ”¯æŒçš„ä½œç‰©: {list(CROP_GROWTH_STAGES.keys())}")
    return CROP_GROWTH_STAGES[crop_name]


def create_fixed_windows(window_sizes: list, n_timesteps: int) -> Dict[str, tuple]:
    """
    åˆ›å»ºå›ºå®šå¤§å°çš„æ—¶é—´çª—å£
    
    Parameters:
        window_sizes: çª—å£å¤§å°åˆ—è¡¨ï¼Œå¦‚ [30, 60, 90]
        n_timesteps: æ€»æ—¶é—´æ­¥æ•°
    
    Returns:
        Dict: çª—å£åç§°åˆ° (å¼€å§‹, ç»“æŸ) çš„æ˜ å°„
    """
    windows_dict = {}
    for window_size in sorted(window_sizes):
        n_windows = n_timesteps // window_size
        
        for i in range(n_windows):
            start = i * window_size
            end = (i + 1) * window_size
            stage_name = f"window_{window_size}d_seg{i+1}"
            windows_dict[stage_name] = (start, end)
        
        # å¤„ç†ä½™æ•°
        remainder = n_timesteps % window_size
        if remainder > 0:
            stage_name = f"window_{window_size}d_remainder"
            windows_dict[stage_name] = (n_windows * window_size, n_timesteps)
    
    return windows_dict


def add_custom_growth_stages(crop_name: str, stages: Dict[str, tuple]) -> None:
    """
    æ·»åŠ è‡ªå®šä¹‰ä½œç‰©ç”Ÿè‚²æœŸ
    
    Parameters:
        crop_name: ä½œç‰©åç§°
        stages: ç”Ÿè‚²æœŸå®šä¹‰ {'stage_name': (start_day, end_day), ...}
    """
    CROP_GROWTH_STAGES[crop_name.lower()] = stages
    print(f"âœ“ å·²æ·»åŠ ä½œç‰© {crop_name} çš„ç”Ÿè‚²æœŸå®šä¹‰")
