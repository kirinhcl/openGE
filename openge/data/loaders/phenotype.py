"""Loader for phenotype/trait data."""

import numpy as np
import pandas as pd
from typing import Union, Dict, List, Tuple, Optional
from pathlib import Path
from datetime import datetime


class PhenotypeLoader:
    """Loader for phenotype/trait data (target variables)."""
    
    # å®šä¹‰å¯ç”¨çš„è¡¨å‹æ€§çŠ¶åŠå…¶å…ƒæ•°æ®
    TRAIT_METADATA = {
        'Yield_Mg_ha': {
            'name': 'Grain Yield',
            'unit': 'Mg/ha',
            'description': 'ç±½ç²’äº§é‡',
            'type': 'continuous',
            'range': (0, 30),
            'primary': True
        },
        'Plant_Height_cm': {
            'name': 'Plant Height',
            'unit': 'cm',
            'description': 'æ ªé«˜',
            'type': 'continuous',
            'range': (50, 400)
        },
        'Ear_Height_cm': {
            'name': 'Ear Height',
            'unit': 'cm',
            'description': 'ç©—é«˜',
            'type': 'continuous',
            'range': (30, 250)
        },
        'Pollen_DAP_days': {
            'name': 'Days to Pollen',
            'unit': 'days',
            'description': 'å¼€èŠ±æœŸ',
            'type': 'continuous',
            'range': (40, 100)
        },
        'Silk_DAP_days': {
            'name': 'Days to Silk',
            'unit': 'days',
            'description': 'åä¸æœŸ',
            'type': 'continuous',
            'range': (40, 100)
        },
        'Grain_Moisture': {
            'name': 'Grain Moisture',
            'unit': '%',
            'description': 'ç±½ç²’æ°´åˆ†',
            'type': 'continuous',
            'range': (10, 40)
        },
        'Twt_kg_m3': {
            'name': 'Test Weight',
            'unit': 'kg/mÂ³',
            'description': 'å®¹é‡',
            'type': 'continuous',
            'range': (500, 850)
        },
        'Stand_Count_plants': {
            'name': 'Stand Count',
            'unit': 'plants',
            'description': 'æˆæ ªæ•°',
            'type': 'count',
            'range': (0, 100)
        },
        'Root_Lodging_plants': {
            'name': 'Root Lodging',
            'unit': 'plants',
            'description': 'æ ¹å€’ä¼æ ªæ•°',
            'type': 'count',
            'range': (0, 100)
        },
        'Stalk_Lodging_plants': {
            'name': 'Stalk Lodging',
            'unit': 'plants',
            'description': 'èŒå€’ä¼æ ªæ•°',
            'type': 'count',
            'range': (0, 100)
        }
    }
    
    def __init__(self):
        """Initialize phenotype data loader."""
        self.data: Optional[pd.DataFrame] = None
        self.trait_names: Optional[List[str]] = None
        self.sample_ids: Optional[List[str]] = None
        self.metadata: Optional[Dict] = None
    
    def load_trait_data(self, 
                       filepath: str,
                       traits: Optional[List[str]] = None,
                       sample_id_col: str = 'Hybrid',
                       env_col: str = 'Env',
                       handle_missing: str = 'drop',
                       filter_by_env: Optional[List[str]] = None) -> Tuple[np.ndarray, List[str], List[str]]:
        """
        Load trait/phenotype data from CSV file.
        
        Args:
            filepath: Path to trait data file
            traits: List of trait names to load. If None, loads all available traits
            sample_id_col: Column name for sample IDs (default: 'Hybrid')
            env_col: Column name for environment IDs (default: 'Env')
            handle_missing: How to handle missing values
                           'drop': Drop samples with any missing traits
                           'mean': Fill with mean value
                           'median': Fill with median value
                           'keep': Keep NaN values
            filter_by_env: List of environments to include (None = all)
            
        Returns:
            tuple: (trait_matrix, sample_ids, trait_names)
                   trait_matrix shape: (n_samples, n_traits)
        """
        print(f"\n{'=' * 70}")
        print(f"åŠ è½½è¡¨å‹æ•°æ®: {Path(filepath).name}")
        print(f"{'=' * 70}")
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(filepath)
        print(f"âœ“ æ–‡ä»¶åŠ è½½æˆåŠŸ: {df.shape}")
        
        # è¿‡æ»¤ç¯å¢ƒ
        if filter_by_env is not None:
            df = df[df[env_col].isin(filter_by_env)]
            print(f"âœ“ è¿‡æ»¤ç¯å¢ƒ: {len(filter_by_env)} ä¸ªç¯å¢ƒ, {len(df)} è¡Œæ•°æ®")
        
        # ç¡®å®šè¦åŠ è½½çš„æ€§çŠ¶
        if traits is None:
            traits = [t for t in self.TRAIT_METADATA.keys() if t in df.columns]
        else:
            # éªŒè¯æ€§çŠ¶æ˜¯å¦å­˜åœ¨
            missing_traits = [t for t in traits if t not in df.columns]
            if missing_traits:
                raise ValueError(f"æœªæ‰¾åˆ°æ€§çŠ¶: {missing_traits}")
        
        print(f"âœ“ ç›®æ ‡æ€§çŠ¶: {len(traits)} ä¸ª")
        for trait in traits:
            meta = self.TRAIT_METADATA.get(trait, {})
            print(f"  - {trait}: {meta.get('description', 'N/A')} ({meta.get('unit', 'N/A')})")
        
        # æå–æ ·æœ¬ID
        if sample_id_col not in df.columns:
            raise ValueError(f"æœªæ‰¾åˆ°æ ·æœ¬IDåˆ—: {sample_id_col}")
        
        # æ„å»ºæ ·æœ¬IDï¼ˆç¯å¢ƒ+æ‚äº¤ç»„åˆï¼‰
        sample_ids = (df[env_col].astype(str) + '_' + df[sample_id_col].astype(str)).tolist()
        print(f"âœ“ æ ·æœ¬æ•°: {len(sample_ids)} (ç¯å¢ƒ_æ‚äº¤ç»„åˆ)")
        
        # æå–æ€§çŠ¶æ•°æ®
        trait_data = df[traits].values
        
        # ç»Ÿè®¡ç¼ºå¤±å€¼
        n_missing = np.isnan(trait_data.astype(float)).sum()
        missing_rate = 100 * n_missing / trait_data.size
        print(f"\nğŸ“Š ç¼ºå¤±å€¼åˆ†æ:")
        print(f"   - æ€»ç¼ºå¤±å€¼: {n_missing} / {trait_data.size} ({missing_rate:.2f}%)")
        
        # æŒ‰æ€§çŠ¶ç»Ÿè®¡ç¼ºå¤±ç‡
        for i, trait in enumerate(traits):
            trait_missing = np.isnan(trait_data[:, i].astype(float)).sum()
            trait_missing_rate = 100 * trait_missing / len(trait_data)
            print(f"   - {trait}: {trait_missing} / {len(trait_data)} ({trait_missing_rate:.2f}%)")
        
        # å¤„ç†ç¼ºå¤±å€¼
        original_size = len(trait_data)
        
        if handle_missing == 'drop':
            # åˆ é™¤æœ‰ç¼ºå¤±å€¼çš„æ ·æœ¬
            valid_mask = ~np.isnan(trait_data.astype(float)).any(axis=1)
            trait_data = trait_data[valid_mask]
            sample_ids = [sid for sid, valid in zip(sample_ids, valid_mask) if valid]
            print(f"   - åˆ é™¤ç¼ºå¤±æ ·æœ¬: {original_size - len(trait_data)} ä¸ª")
            
        elif handle_missing == 'mean':
            # ç”¨å‡å€¼å¡«å……
            trait_data = trait_data.astype(float)
            for i in range(trait_data.shape[1]):
                col_mean = np.nanmean(trait_data[:, i])
                trait_data[np.isnan(trait_data[:, i]), i] = col_mean
            print(f"   - ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼")
            
        elif handle_missing == 'median':
            # ç”¨ä¸­ä½æ•°å¡«å……
            trait_data = trait_data.astype(float)
            for i in range(trait_data.shape[1]):
                col_median = np.nanmedian(trait_data[:, i])
                trait_data[np.isnan(trait_data[:, i]), i] = col_median
            print(f"   - ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼")
            
        elif handle_missing == 'keep':
            trait_data = trait_data.astype(float)
            print(f"   - ä¿ç•™ç¼ºå¤±å€¼")
        
        else:
            raise ValueError(f"æœªçŸ¥çš„ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•: {handle_missing}")
        
        # ç»Ÿè®¡æ¯ä¸ªæ€§çŠ¶
        print(f"\nâœ“ æ€§çŠ¶ç»Ÿè®¡:")
        trait_data_float = trait_data.astype(float)
        for i, trait in enumerate(traits):
            valid_data = trait_data_float[:, i][~np.isnan(trait_data_float[:, i])]
            if len(valid_data) > 0:
                print(f"   - {trait}:")
                print(f"     å‡å€¼={valid_data.mean():.2f}, "
                      f"æ ‡å‡†å·®={valid_data.std():.2f}, "
                      f"èŒƒå›´=[{valid_data.min():.2f}, {valid_data.max():.2f}]")
        
        # ä¿å­˜å…ƒæ•°æ®
        self.data = df
        self.trait_names = traits
        self.sample_ids = sample_ids
        
        print(f"\nâœ“ æœ€ç»ˆæ•°æ®å½¢çŠ¶: {trait_data.shape}")
        print(f"{'=' * 70}")
        print(f"è¡¨å‹æ•°æ®åŠ è½½å®Œæˆ")
        print(f"{'=' * 70}\n")
        
        return trait_data.astype(np.float32), sample_ids, traits
    
    def handle_outliers(self, 
                       data: np.ndarray,
                       trait_names: Optional[List[str]] = None,
                       method: str = "iqr",
                       iqr_factor: float = 1.5,
                       zscore_threshold: float = 3.0,
                       replace_with: str = 'median') -> Tuple[np.ndarray, Dict]:
        """
        Handle outliers in phenotype data.
        
        Args:
            data: Phenotype data (n_samples, n_traits)
            trait_names: Names of traits (for logging)
            method: Outlier detection method
                   'iqr': Interquartile range method
                   'zscore': Z-score method
                   'range': Use predefined valid ranges from TRAIT_METADATA
            iqr_factor: IQR multiplier for outlier detection (default: 1.5)
            zscore_threshold: Z-score threshold (default: 3.0)
            replace_with: How to replace outliers ('median', 'mean', 'clip', 'nan')
            
        Returns:
            tuple: (cleaned_data, outlier_info)
        """
        print(f"\n{'=' * 70}")
        print(f"å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç† (æ–¹æ³•: {method})")
        print(f"{'=' * 70}")
        
        cleaned_data = data.copy().astype(float)
        outlier_info = {'method': method, 'traits': {}}
        
        for i in range(data.shape[1]):
            trait_name = trait_names[i] if trait_names and i < len(trait_names) else f"Trait_{i}"
            col_data = data[:, i].astype(float)
            valid_mask = ~np.isnan(col_data)
            valid_data = col_data[valid_mask]
            
            if len(valid_data) == 0:
                continue
            
            outlier_mask = np.zeros(len(col_data), dtype=bool)
            
            if method == 'iqr':
                # IQRæ–¹æ³•
                q1 = np.percentile(valid_data, 25)
                q3 = np.percentile(valid_data, 75)
                iqr = q3 - q1
                lower_bound = q1 - iqr_factor * iqr
                upper_bound = q3 + iqr_factor * iqr
                
                outlier_mask = valid_mask & ((col_data < lower_bound) | (col_data > upper_bound))
                bounds = (lower_bound, upper_bound)
                
            elif method == 'zscore':
                # Z-scoreæ–¹æ³•
                mean = valid_data.mean()
                std = valid_data.std()
                if std > 0:
                    z_scores = np.abs((col_data - mean) / std)
                    outlier_mask = valid_mask & (z_scores > zscore_threshold)
                bounds = (mean - zscore_threshold * std, mean + zscore_threshold * std)
                
            elif method == 'range':
                # ä½¿ç”¨é¢„å®šä¹‰èŒƒå›´
                if trait_name in self.TRAIT_METADATA:
                    valid_range = self.TRAIT_METADATA[trait_name]['range']
                    outlier_mask = valid_mask & ((col_data < valid_range[0]) | (col_data > valid_range[1]))
                    bounds = valid_range
                else:
                    # å¦‚æœæ²¡æœ‰é¢„å®šä¹‰èŒƒå›´ï¼Œä½¿ç”¨IQR
                    q1 = np.percentile(valid_data, 25)
                    q3 = np.percentile(valid_data, 75)
                    iqr = q3 - q1
                    bounds = (q1 - 1.5 * iqr, q3 + 1.5 * iqr)
                    outlier_mask = valid_mask & ((col_data < bounds[0]) | (col_data > bounds[1]))
            
            else:
                raise ValueError(f"æœªçŸ¥çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•: {method}")
            
            n_outliers = outlier_mask.sum()
            outlier_info['traits'][trait_name] = {
                'n_outliers': int(n_outliers),
                'outlier_rate': float(n_outliers / valid_mask.sum() * 100),
                'bounds': bounds
            }
            
            # æ›¿æ¢å¼‚å¸¸å€¼
            if n_outliers > 0:
                if replace_with == 'median':
                    replacement = np.median(valid_data[~outlier_mask[valid_mask]])
                elif replace_with == 'mean':
                    replacement = np.mean(valid_data[~outlier_mask[valid_mask]])
                elif replace_with == 'clip':
                    cleaned_data[outlier_mask & (col_data < bounds[0]), i] = bounds[0]
                    cleaned_data[outlier_mask & (col_data > bounds[1]), i] = bounds[1]
                    replacement = None
                elif replace_with == 'nan':
                    cleaned_data[outlier_mask, i] = np.nan
                    replacement = None
                else:
                    raise ValueError(f"æœªçŸ¥çš„æ›¿æ¢æ–¹æ³•: {replace_with}")
                
                if replacement is not None and replace_with not in ['clip', 'nan']:
                    cleaned_data[outlier_mask, i] = replacement
                
                print(f"âœ“ {trait_name}: {n_outliers} ä¸ªå¼‚å¸¸å€¼ "
                      f"({n_outliers / valid_mask.sum() * 100:.2f}%), "
                      f"èŒƒå›´: [{bounds[0]:.2f}, {bounds[1]:.2f}]")
        
        print(f"{'=' * 70}\n")
        return cleaned_data.astype(np.float32), outlier_info
    
    def get_trait_info(self, trait_names: Optional[List[str]] = None) -> Dict:
        """
        Get metadata about traits.
        
        Args:
            trait_names: List of trait names. If None, returns all available traits.
            
        Returns:
            Dictionary with trait information (name, unit, description, etc.)
        """
        if trait_names is None:
            return self.TRAIT_METADATA.copy()
        
        return {name: self.TRAIT_METADATA[name] for name in trait_names 
                if name in self.TRAIT_METADATA}
    
    def save_to_numpy(self,
                     trait_data: np.ndarray,
                     sample_ids: List[str],
                     trait_names: List[str],
                     output_path: str):
        """
        Save phenotype data to .npz file.
        
        Args:
            trait_data: Trait/phenotype data
            sample_ids: Sample IDs
            trait_names: Trait names
            output_path: Output file path
        """
        np.savez_compressed(
            output_path,
            traits=trait_data,
            sample_ids=sample_ids,
            trait_names=trait_names
        )
        
        file_size = Path(output_path).stat().st_size / (1024 * 1024)  # MB
        print(f"âœ“ ä¿å­˜è¡¨å‹æ•°æ®: {output_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    def compute_correlations(self, 
                            trait_data: np.ndarray,
                            trait_names: List[str]) -> Tuple[np.ndarray, pd.DataFrame]:
        """
        Compute correlation matrix between traits.
        
        Args:
            trait_data: Trait data (n_samples, n_traits)
            trait_names: List of trait names
            
        Returns:
            tuple: (correlation_matrix, correlation_dataframe)
        """
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = np.corrcoef(trait_data.T)
        
        # åˆ›å»ºDataFrameä¾¿äºæŸ¥çœ‹
        corr_df = pd.DataFrame(corr_matrix, 
                              index=trait_names,
                              columns=trait_names)
        
        return corr_matrix, corr_df
