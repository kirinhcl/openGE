"""Loader for environmental data (weather, soil, EC data)."""

import numpy as np
import pandas as pd
from typing import List, Union, Tuple, Optional
from pathlib import Path

# ä» preprocess æ¨¡å—å¯¼å…¥é¢„å¤„ç†å‡½æ•°
from openge.data.preprocess import (
    check_and_handle_missing,
    aggregate_temporal_to_static,
    aggregate_temporal_features
)


class EnvironmentLoader:
    """Loader for environmental data (weather, soil, ec)."""
    
    def __init__(self):
        """Initialize environment data loader."""
        self.feature_names: Optional[List[str]] = None
    
    def _detect_data_format(self, data: Union[np.ndarray, pd.DataFrame]) -> str:
        """Detect if data is static or temporal."""
        if isinstance(data, pd.DataFrame):
            temporal_cols = [col for col in data.columns 
                           if any(keyword in col.lower() for keyword in ['date', 'time', 'day'])]
            return 'temporal' if temporal_cols else 'static'
        
        elif isinstance(data, np.ndarray):
            return 'temporal' if data.ndim == 3 else 'static'
        
        return 'static'
    
    def _is_long_format(self, df: pd.DataFrame) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯é•¿æ ¼å¼"""
        cols_lower = set(df.columns.str.lower())
        has_sample_col = any(col in cols_lower for col in ['sample_id', 'env', 'location'])
        has_feature_col = any(col in cols_lower for col in ['feature', 'variable', 'value'])
        
        return has_sample_col and has_feature_col
    
    def _is_temporal_wide_format(self, df: pd.DataFrame, 
                                sample_col: str = "Env",
                                date_col: str = "Date") -> Tuple[bool, Optional[str], Optional[str]]:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´åºåˆ—çš„å®½æ ¼å¼ï¼Œå¹¶è¿”å›å®é™…çš„åˆ—åã€‚
        
        Returns:
            (is_temporal, actual_sample_col, actual_date_col)
        """
        cols_lower = {col.lower(): col for col in df.columns}
        sample_col_lower = sample_col.lower()
        date_col_lower = date_col.lower()
        
        if sample_col_lower in cols_lower and date_col_lower in cols_lower:
            return True, cols_lower[sample_col_lower], cols_lower[date_col_lower]
        
        return False, None, None
    
    def _convert_long_to_wide(self, df: pd.DataFrame) -> pd.DataFrame:
        """è½¬æ¢é•¿æ ¼å¼åˆ°å®½æ ¼å¼"""
        cols_lower = {col.lower(): col for col in df.columns}
        
        # è¯†åˆ«å…³é”®åˆ—
        sample_col = cols_lower.get('sample_id') or cols_lower.get('env') or cols_lower.get('location')
        feature_col = cols_lower.get('feature') or cols_lower.get('variable')
        value_col = cols_lower.get('value')
        date_col = cols_lower.get('date')
        
        if not (sample_col and feature_col and value_col):
            print("âš ï¸ è­¦å‘Šï¼šç¼ºå°‘å¿…éœ€åˆ—ï¼Œæ— æ³•è½¬æ¢é•¿æ ¼å¼")
            return df
        
        try:
            if date_col:
                result = df.pivot_table(
                    index=[sample_col, date_col],
                    columns=feature_col,
                    values=value_col,
                    aggfunc='first'
                )
            else:
                result = df.pivot_table(
                    index=sample_col,
                    columns=feature_col,
                    values=value_col,
                    aggfunc='first'
                )
            return result.reset_index()
        except Exception as e:
            print(f"âš ï¸ è½¬æ¢å¤±è´¥: {e}")
            return df
    
    def _ensure_datetime_column(self, df: pd.DataFrame, date_col: str) -> pd.DataFrame:
        """
        ç¡®ä¿æ—¥æœŸåˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®

        Parameters:
            df: è¾“å…¥ DataFrame
            date_col: æ—¥æœŸåˆ—å

        Returns:
            pd.DataFrame: æ—¥æœŸåˆ—å·²è½¬æ¢çš„ DataFrame
        """
        if date_col not in df.columns:
            raise ValueError(f"åˆ— '{date_col}' ä¸å­˜åœ¨")
        
        df = df.copy()
        
        # æ£€æŸ¥æ—¥æœŸåˆ—çš„å½“å‰æ•°æ®ç±»å‹
        current_dtype = df[date_col].dtype
        print(f"\nğŸ“… æ—¥æœŸåˆ— '{date_col}' å¤„ç†:")
        print(f"   åŸå§‹æ•°æ®ç±»å‹: {current_dtype}")
        
        # è·å–æ ·æœ¬å€¼ç”¨äºè¯Šæ–­
        sample_values = df[date_col].head(3).tolist()
        print(f"   æ ·æœ¬å€¼: {sample_values}")

        # å¦‚æœå·²ç»æ˜¯ datetime ç±»å‹ï¼Œç›´æ¥è¿”å›
        if pd.api.types.is_datetime64_any_dtype(current_dtype):
            print(f"   âœ“ å·²ç»æ˜¯ datetime ç±»å‹ï¼Œæ— éœ€è½¬æ¢")
            return df

        # å°è¯•è½¬æ¢ä¸º datetime
        try:
            first_value = df[date_col].iloc[0]
            
            # âœ… ã€ä¿®å¤ã€‘æ£€æµ‹æ•´æ•°æ ¼å¼çš„æ—¥æœŸï¼ˆå¦‚ 20240411ï¼‰
            if pd.api.types.is_integer_dtype(current_dtype):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ YYYYMMDD æ ¼å¼ï¼ˆ8ä½æ•´æ•°ï¼‰
                if 10000000 <= first_value <= 99999999:
                    print(f"   æ£€æµ‹åˆ° YYYYMMDD æ•´æ•°æ ¼å¼")
                    df[date_col] = pd.to_datetime(df[date_col].astype(str), format='%Y%m%d')
                # æ£€æŸ¥æ˜¯å¦æ˜¯ YYMMDD æ ¼å¼ï¼ˆ6ä½æ•´æ•°ï¼‰
                elif 100000 <= first_value <= 999999:
                    print(f"   æ£€æµ‹åˆ° YYMMDD æ•´æ•°æ ¼å¼")
                    df[date_col] = pd.to_datetime(df[date_col].astype(str), format='%y%m%d')
                else:
                    # å°è¯•è‡ªåŠ¨è§£æ
                    df[date_col] = pd.to_datetime(df[date_col].astype(str))
            
            # âœ… ã€ä¿®å¤ã€‘æ£€æµ‹å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸ
            elif pd.api.types.is_string_dtype(current_dtype) or current_dtype == object:
                first_str = str(first_value)
                
                # å°è¯•å¸¸è§çš„æ—¥æœŸæ ¼å¼
                date_formats = [
                    ('%Y%m%d', '20240411'),           # YYYYMMDD
                    ('%Y-%m-%d', '2024-04-11'),       # YYYY-MM-DD
                    ('%Y/%m/%d', '2024/04/11'),       # YYYY/MM/DD
                    ('%d-%m-%Y', '11-04-2024'),       # DD-MM-YYYY
                    ('%d/%m/%Y', '11/04/2024'),       # DD/MM/YYYY
                    ('%m-%d-%Y', '04-11-2024'),       # MM-DD-YYYY
                    ('%m/%d/%Y', '04/11/2024'),       # MM/DD/YYYY
                    ('%Y-%m-%d %H:%M:%S', '2024-04-11 00:00:00'),  # with time
                ]
                
                converted = False
                for fmt, example in date_formats:
                    try:
                        df[date_col] = pd.to_datetime(df[date_col], format=fmt)
                        print(f"   ä½¿ç”¨æ ¼å¼ '{fmt}' è½¬æ¢æˆåŠŸ")
                        converted = True
                        break
                    except (ValueError, TypeError):
                        continue
                
                # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨è§£æ
                if not converted:
                    print(f"   å°è¯•è‡ªåŠ¨è§£ææ—¥æœŸæ ¼å¼...")
                    df[date_col] = pd.to_datetime(df[date_col], infer_datetime_format=True)
            
            else:
                # å…¶ä»–ç±»å‹ï¼Œå°è¯•è‡ªåŠ¨è§£æ
                df[date_col] = pd.to_datetime(df[date_col])
            
            print(f"   âœ“ æˆåŠŸè½¬æ¢ä¸º datetime ç±»å‹")
            
            # æ˜¾ç¤ºæ—¥æœŸèŒƒå›´
            date_min = df[date_col].min()
            date_max = df[date_col].max()
            print(f"   âœ“ æ—¥æœŸèŒƒå›´: {date_min.strftime('%Y-%m-%d')} åˆ° {date_max.strftime('%Y-%m-%d')}")
            
            # âœ… ã€æ–°å¢ã€‘éªŒè¯æ—¥æœŸæ˜¯å¦åˆç†ï¼ˆ1900-2100å¹´ä¹‹é—´ï¼‰
            if date_min.year < 1900 or date_max.year > 2100:
                print(f"   âŒ è­¦å‘Šï¼šæ—¥æœŸèŒƒå›´å¼‚å¸¸ï¼å¯èƒ½è§£æé”™è¯¯")
                raise ValueError(f"æ—¥æœŸèŒƒå›´å¼‚å¸¸: {date_min} åˆ° {date_max}")
            
            return df

        except Exception as e:
            print(f"   âŒ è½¬æ¢å¤±è´¥: {e}")
            print(f"   æ ·æœ¬å€¼: {df[date_col].head().tolist()}")
            raise ValueError(f"æ— æ³•å°†åˆ— '{date_col}' è½¬æ¢ä¸º datetime ç±»å‹: {e}")
    
    def _process_temporal_weather(self, df: pd.DataFrame,
                                 sample_col: str,
                                 date_col: str,
                                 reshape_to_temporal: bool) -> pd.DataFrame:
        """
        å¤„ç†æ—¶é—´åºåˆ—å¤©æ°”æ•°æ®ï¼Œè¿”å› DataFrame
        """
        # éªŒè¯åˆ—åå­˜åœ¨
        if sample_col not in df.columns:
            raise ValueError(f"åˆ— '{sample_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(df.columns)}")
        if date_col not in df.columns:
            raise ValueError(f"åˆ— '{date_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(df.columns)}")
        
        # éªŒè¯å’Œè½¬æ¢æ—¥æœŸåˆ—
        df = self._ensure_datetime_column(df, date_col)
        
        feature_cols = [col for col in df.columns 
                       if col not in [sample_col, date_col]]
        
        if not feature_cols:
            raise ValueError(f"æ²¡æœ‰ç‰¹å¾åˆ—ï¼æ’é™¤äº† {sample_col} å’Œ {date_col} åæ²¡æœ‰å…¶ä»–åˆ—")
        
        grouped_data = []
        sample_ids = []
        
        for sample_id, group in df.groupby(sample_col):
            try:
                group_sorted = group.sort_values(date_col)
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼šæ— æ³•æŒ‰ '{date_col}' æ’åºæ ·æœ¬ '{sample_id}': {e}")
                raise
        
            features = group_sorted[feature_cols].values
            grouped_data.append(features)
            sample_ids.append(sample_id)
        
        timesteps = [len(t) for t in grouped_data]
        
        if reshape_to_temporal:
            result_df = df.copy()
            return result_df
            
        else:
            if len(set(timesteps)) > 1:
                print(f"\nâš ï¸ è­¦å‘Šï¼šæ ·æœ¬çš„æ—¶é—´æ­¥æ•°ä¸åŒï¼š{set(timesteps)}")
                print(f"   æ ·æœ¬æ—¶é—´æ­¥æ•°è¯¦æƒ…ï¼š")
                for sample_id, n_steps in zip(sample_ids, timesteps):
                    print(f"      â€¢ {sample_id}: {n_steps} æ­¥")
                
                max_timesteps = max(timesteps)
                min_timesteps = min(timesteps)
                
                print(f"\n   å¤„ç†æ–¹æ¡ˆï¼š")
                print(f"   1. åˆ é™¤æ—¶é—´æ­¥æ•°è¿‡å°‘çš„æ ·æœ¬")
                print(f"   2. æˆªæ–­åˆ°æœ€å°æ—¶é—´æ­¥æ•° ({min_timesteps})")
                print(f"   3. å¡«å……æœ€åå€¼åˆ°æœ€å¤§æ—¶é—´æ­¥æ•° ({max_timesteps}) â­ é»˜è®¤")
                print(f"\n   é‡‡ç”¨æ–¹æ¡ˆ3ï¼šå¡«å……æœ€åå€¼åˆ°æœ€å¤§æ—¶é—´æ­¥æ•° ({max_timesteps})")
                
                padded_data = []
                for i, data in enumerate(grouped_data):
                    if len(data) < max_timesteps:
                        last_row = data[-1:, :]
                        n_pad = max_timesteps - len(data)
                        pad_rows = np.repeat(last_row, n_pad, axis=0)
                        padded = np.vstack([data, pad_rows])
                        padded_data.append(padded)
                        print(f"   âœ“ {sample_ids[i]}: {len(data)} â†’ {max_timesteps} æ­¥ (å¡«å…… {n_pad} è¡Œ)")
                    else:
                        padded_data.append(data)
                
                grouped_data = padded_data
                print(f"   âœ“ æ‰€æœ‰æ ·æœ¬å·²å¡«å……åˆ° {max_timesteps} æ­¥")
            
            temporal_array = np.array(grouped_data)
            
            n_nan_before = np.isnan(temporal_array).sum()
            if n_nan_before > 0:
                print(f"\nâš ï¸ è­¦å‘Šï¼šåŸå§‹æ•°æ®ä¸­æœ‰ {n_nan_before} ä¸ª NaNï¼Œä¼šè¢«å¿½ç•¥")
            
            aggregated = aggregate_temporal_to_static(temporal_array)
            
            n_features = len(feature_cols)
            feature_names = []
            for feat in feature_cols:
                feature_names.extend([f"{feat}_mean", f"{feat}_max", f"{feat}_min", f"{feat}_std"])
            
            result_df = pd.DataFrame(aggregated, columns=feature_names)
            result_df.insert(0, sample_col, sample_ids)
            
            return result_df

    def load_weather_data(self, filepath: str, 
                         reshape_to_temporal: bool = False,
                         sample_col: str = "Env",
                         date_col: str = "Date",
                         handle_missing: str = 'drop',
                         missing_threshold: float = 0.5) -> pd.DataFrame:
        """åŠ è½½å¤©æ°”æ•°æ®ï¼Œè¿”å› DataFrame"""
        path = Path(filepath)
        
        try:
            if path.suffix == ".csv":
                data = pd.read_csv(filepath)
            elif path.suffix in [".xlsx", ".xls"]:
                data = pd.read_excel(filepath)
            elif path.suffix == ".npz":
                loaded = np.load(filepath)
                weather_array = loaded["weather"].astype(np.float32)
                print("âœ“ NPZ æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(weather_array)
            elif path.suffix == ".npy":
                weather_array = np.load(filepath).astype(np.float32)
                print("âœ“ NPY æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(weather_array)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
            
            if self._is_long_format(data):
                data = self._convert_long_to_wide(data)
            
            is_temporal, actual_sample_col, actual_date_col = self._is_temporal_wide_format(
                data, sample_col, date_col
            )
            if is_temporal:
                result_df = self._process_temporal_weather(
                    data, actual_sample_col, actual_date_col, reshape_to_temporal
                )
            else:
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if not numeric_cols:
                    raise ValueError("å¤©æ°”æ•°æ®ä¸­æ²¡æœ‰æ•°å€¼åˆ—")
                result_df = data[numeric_cols].copy().astype(np.float32)
            
            result_df = check_and_handle_missing(
                result_df,
                method=handle_missing,
                threshold=missing_threshold,
                name='å¤©æ°”æ•°æ®'
            )
            
            print(f"âœ“ å¤©æ°”æ•°æ®åŠ è½½å®Œæˆ: {result_df.shape}")
            return result_df
    
        except Exception as e:
            print(f"âŒ åŠ è½½å¤©æ°”æ•°æ®å¤±è´¥: {e}")
            raise
    
    def load_soil_data(self, filepath: str,
                      handle_missing: str = 'drop',
                      missing_threshold: float = 0.5,
                      sample_col: str = 'Env',
                      date_col: str = None) -> pd.DataFrame:
        """åŠ è½½åœŸå£¤æ•°æ®ï¼Œè¿”å› DataFrame"""
        path = Path(filepath)
        
        try:
            # ã€æ­¥éª¤1ã€‘åŠ è½½æ–‡ä»¶
            if path.suffix == ".csv":
                data = pd.read_csv(filepath)
            elif path.suffix in [".xlsx", ".xls"]:
                data = pd.read_excel(filepath)
            elif path.suffix == ".npz":
                loaded = np.load(filepath)
                soil_array = loaded["soil"].astype(np.float32)
                print("âœ“ NPZ æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(soil_array)
            elif path.suffix == ".npy":
                soil_array = np.load(filepath).astype(np.float32)
                print("âœ“ NPY æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(soil_array)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
        
            # ã€æ­¥éª¤2ã€‘è‡ªåŠ¨æ£€æµ‹æ—¥æœŸåˆ—
            if date_col is None:
                possible_date_cols = [col for col in data.columns 
                                    if any(keyword in col.lower() for keyword in ['date', 'time', 'day'])]
                if possible_date_cols:
                    date_col = possible_date_cols[0]
                    print(f"ğŸ“… è‡ªåŠ¨æ£€æµ‹åˆ°æ—¥æœŸåˆ—: '{date_col}'")
        
            # ã€æ­¥éª¤3ã€‘ä¿ç•™æ ·æœ¬IDåˆ—ï¼Œæå–æ•°å€¼åˆ—
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if not numeric_cols:
                raise ValueError("åœŸå£¤æ•°æ®ä¸­æ²¡æœ‰æ•°å€¼åˆ—")
        
            # ã€æ­¥éª¤4ã€‘æ„å»ºåŒ…å«æ ·æœ¬IDåˆ—çš„å®Œæ•´ DataFrame
            if sample_col in data.columns:
                result_df = data[[sample_col] + numeric_cols].copy()
                result_df[numeric_cols] = result_df[numeric_cols].astype(np.float32)
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° '{sample_col}' åˆ—")
                result_df = data[numeric_cols].copy().astype(np.float32)
        
            # ã€æ­¥éª¤5ã€‘å¤„ç†ç¼ºå¤±å€¼
            result_df = check_and_handle_missing(
                result_df,
                method=handle_missing,
                threshold=missing_threshold,
                name='åœŸå£¤æ•°æ®'
            )
        
            # ã€æ­¥éª¤6ã€‘ç¡®ä¿æ ·æœ¬IDåˆ—åœ¨ç¬¬ä¸€åˆ—
            if sample_col in result_df.columns:
                cols = [sample_col] + [c for c in result_df.columns if c != sample_col]
                result_df = result_df[cols]
        
            print(f"âœ“ åœŸå£¤æ•°æ®åŠ è½½å®Œæˆ: {result_df.shape}")
            return result_df

        except Exception as e:
            print(f"âŒ åŠ è½½åœŸå£¤æ•°æ®å¤±è´¥: {e}")
            raise

    def load_ec_data(self, filepath: str,
                    handle_missing: str = 'drop',
                    missing_threshold: float = 0.5,
                    sample_col: str = 'Env',
                    date_col: str = None) -> pd.DataFrame:
        """åŠ è½½ EC æ•°æ®ï¼Œè¿”å› DataFrame"""
        path = Path(filepath)
        
        try:
            # ã€æ­¥éª¤1ã€‘åŠ è½½æ–‡ä»¶
            if path.suffix == ".csv":
                data = pd.read_csv(filepath)
            elif path.suffix in [".xlsx", ".xls"]:
                data = pd.read_excel(filepath)
            elif path.suffix == ".npz":
                loaded = np.load(filepath)
                ec_array = loaded["ec"].astype(np.float32)
                print("âœ“ NPZ æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(ec_array)
            elif path.suffix == ".npy":
                ec_array = np.load(filepath).astype(np.float32)
                print("âœ“ NPY æ–‡ä»¶å·²åŠ è½½ï¼ˆæ— åˆ—åä¿¡æ¯ï¼‰")
                return pd.DataFrame(ec_array)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
            
            # ã€æ­¥éª¤2ã€‘è‡ªåŠ¨æ£€æµ‹æ—¥æœŸåˆ—
            if date_col is None:
                possible_date_cols = [col for col in data.columns 
                                    if any(keyword in col.lower() for keyword in ['date', 'time', 'day'])]
                if possible_date_cols:
                    date_col = possible_date_cols[0]
                    print(f"ğŸ“… è‡ªåŠ¨æ£€æµ‹åˆ°æ—¥æœŸåˆ—: '{date_col}'")
            
            # ã€æ­¥éª¤3ã€‘ä¿ç•™æ ·æœ¬IDåˆ—ï¼Œæå–æ•°å€¼åˆ—
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if not numeric_cols:
                raise ValueError("EC æ•°æ®ä¸­æ²¡æœ‰æ•°å€¼åˆ—")
            
            # ã€æ­¥éª¤4ã€‘æ„å»ºåŒ…å«æ ·æœ¬IDåˆ—çš„å®Œæ•´ DataFrame
            if sample_col in data.columns:
                result_df = data[[sample_col] + numeric_cols].copy()
                result_df[numeric_cols] = result_df[numeric_cols].astype(np.float32)
            else:
                print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° '{sample_col}' åˆ—")
                result_df = data[numeric_cols].copy().astype(np.float32)
            
            # ã€æ­¥éª¤5ã€‘å¤„ç†ç¼ºå¤±å€¼
            result_df = check_and_handle_missing(
                result_df,
                method=handle_missing,
                threshold=missing_threshold,
                name='ECæ•°æ®'
            )
            
            # ã€æ­¥éª¤6ã€‘ç¡®ä¿æ ·æœ¬IDåˆ—åœ¨ç¬¬ä¸€åˆ—
            if sample_col in result_df.columns:
                cols = [sample_col] + [c for c in result_df.columns if c != sample_col]
                result_df = result_df[cols]
            
            print(f"âœ“ EC æ•°æ®åŠ è½½å®Œæˆ: {result_df.shape}")
            return result_df

        except Exception as e:
            print(f"âŒ åŠ è½½ EC æ•°æ®å¤±è´¥: {e}")
            raise
    
    def load_all_environment_data(self, 
                                  weather_file: str,
                                  soil_file: str,
                                  ec_file: str,
                                  temporal_weather_file: str = None,
                                  temporal_windows: Union[list, str, dict] = None,
                                  crop_name: str = 'maize',
                                  sample_col: str = 'Env',
                                  handle_missing: str = 'drop',
                                  missing_threshold: float = 0.5) -> pd.DataFrame:
        """ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ç¯å¢ƒæ•°æ®ï¼Œè¿”å›åˆå¹¶åçš„ DataFrame"""
        print(f"æ­£åœ¨åŠ è½½ç¯å¢ƒæ•°æ®...")
        
        df_weather = self.load_weather_data(
            weather_file, 
            sample_col=sample_col,
            handle_missing=handle_missing,
            missing_threshold=missing_threshold
        )
        df_soil = self.load_soil_data(
            soil_file,
            handle_missing=handle_missing,
            missing_threshold=missing_threshold
        )
        df_ec = self.load_ec_data(
            ec_file,
            handle_missing=handle_missing,
            missing_threshold=missing_threshold
        )
        
        print(f"\nâœ“ åŸºç¡€æ•°æ®åŠ è½½å®Œæˆ: weather{df_weather.shape}, soil{df_soil.shape}, ec{df_ec.shape}")
        
        df_combined = self._merge_dataframes(
            [df_weather, df_soil, df_ec], 
            sample_col=sample_col
        )
        
        print(f"âœ“ åŸºç¡€æ•°æ®åˆå¹¶å®Œæˆ: {df_combined.shape}")
        
        if temporal_weather_file is not None and temporal_windows is not None:
            temporal_data = np.load(temporal_weather_file)
            
            if temporal_data.ndim != 3:
                raise ValueError(f"æ—¶é—´åºåˆ—æ•°æ®åº”ä¸º 3Dï¼Œä½†å¾—åˆ° {temporal_data.ndim}D")
            
            n_samples_combined = len(df_combined)
            n_samples_temporal = temporal_data.shape[0]
            if n_samples_temporal != n_samples_combined:
                raise ValueError(f"æ—¶é—´åºåˆ—æ•°æ®æ ·æœ¬æ•°ä¸åŒ¹é…ï¼")
            
            aggregated, feature_names = aggregate_temporal_features(
                temporal_data, temporal_windows, crop_name, return_feature_names=True
            )
            
            df_temporal = pd.DataFrame(aggregated, columns=feature_names)
            df_combined = pd.concat([df_combined, df_temporal], axis=1)
            print(f"âœ“ æ—¶é—´èšåˆå®Œæˆ: æ·»åŠ  {df_temporal.shape[1]} åˆ—")
        
        df_combined = check_and_handle_missing(
            df_combined,
            method=handle_missing,
            threshold=missing_threshold,
            name='åˆå¹¶åçš„ç¯å¢ƒæ•°æ®'
        )
        
        print(f"âœ“ åˆå¹¶å®Œæˆ: {df_combined.shape}\n")
        return df_combined

    def _merge_dataframes(self, 
                         dataframes: List[pd.DataFrame],
                         sample_col: str = 'Env',
                         how: str = 'inner') -> pd.DataFrame:
        """æŒ‰æ ·æœ¬IDåˆ—åˆå¹¶å¤šä¸ª DataFrame"""
        if not dataframes:
            raise ValueError("dataframes åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        dfs_with_col = []
        dfs_without_col = []
        
        for i, df in enumerate(dataframes):
            if sample_col in df.columns:
                dfs_with_col.append((i, df))
            else:
                dfs_without_col.append((i, df))
        
        if dfs_without_col:
            print(f"âš ï¸ è­¦å‘Šï¼šä»¥ä¸‹ DataFrame æ²¡æœ‰ '{sample_col}' åˆ—ï¼Œå°†ä½¿ç”¨è¡Œç´¢å¼•å¯¹é½:")
            for idx, _ in dfs_without_col:
                print(f"   - dataframes[{idx}]")
        
        if not dfs_with_col:
            print(f"âŒ é”™è¯¯ï¼šæ‰€æœ‰ DataFrame éƒ½æ²¡æœ‰ '{sample_col}' åˆ—ï¼")
            return pd.concat(dataframes, axis=1)
        
        result = dfs_with_col[0][1].copy()
        
        for i in range(1, len(dfs_with_col)):
            other_df = dfs_with_col[i][1]
            
            overlap_cols = set(result.columns) & set(other_df.columns)
            overlap_cols.discard(sample_col)
            
            if overlap_cols:
                print(f"âš ï¸ è­¦å‘Šï¼šå‘ç°é‡å¤åˆ—å {overlap_cols}")
            
            result = result.merge(other_df, on=sample_col, how=how)
        
        for idx, df_no_col in dfs_without_col:
            if len(df_no_col) != len(result):
                print(f"âš ï¸ è­¦å‘Šï¼šdataframes[{idx}] çš„æ ·æœ¬æ•°ä¸åŒ¹é…")
            
            df_no_col_reset = df_no_col.reset_index(drop=True)
            result_reset = result.reset_index(drop=True)
            result = pd.concat([result_reset, df_no_col_reset], axis=1)
        
        n_missing = result.isna().sum().sum()
        if n_missing > 0:
            print(f"âš ï¸ è­¦å‘Šï¼šåˆå¹¶åæœ‰ {n_missing} ä¸ªç¼ºå¤±å€¼")
        
        return result
    
    def convert_to_3d_array(self, df: pd.DataFrame, 
                           sample_col: str = 'Env', 
                           date_col: str = 'Date') -> Tuple[np.ndarray, List[str], List[str]]:
        """
        å°†é•¿æ ¼å¼ DataFrame è½¬æ¢ä¸º 3D NumPy æ•°ç»„
        
        Parameters:
            df: é•¿æ ¼å¼å¤©æ°”æ•°æ® DataFrame
            sample_col: æ ·æœ¬IDåˆ—å
            date_col: æ—¥æœŸåˆ—å
        
        Returns:
            tuple: (weather_3d, sample_ids, feature_names)
                   weather_3d å½¢çŠ¶: (n_samples, n_timesteps, n_features)
        """
        # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ·æœ¬IDå’Œæ—¥æœŸï¼‰
        feature_cols = [col for col in df.columns 
                       if col not in [sample_col, date_col]]
        
        if not feature_cols:
            raise ValueError(f"æ²¡æœ‰ç‰¹å¾åˆ—ï¼æ’é™¤äº† {sample_col} å’Œ {date_col} åæ²¡æœ‰å…¶ä»–åˆ—")
        
        # æŒ‰æ ·æœ¬åˆ†ç»„å¹¶æ’åº
        grouped_data = []
        sample_ids = []
        
        for sample_id, group in df.groupby(sample_col):
            group_sorted = group.sort_values(date_col)
            features = group_sorted[feature_cols].values
            grouped_data.append(features)
            sample_ids.append(sample_id)
        
        # æ£€æŸ¥æ—¶é—´æ­¥æ•°æ˜¯å¦ä¸€è‡´
        timesteps = [len(t) for t in grouped_data]
        if len(set(timesteps)) > 1:
            print(f"\nâš ï¸ æ—¶é—´æ­¥æ•°ä¸ä¸€è‡´: {set(timesteps)}")
            # å¡«å……åˆ°æœ€å¤§æ—¶é—´æ­¥æ•°
            max_t = max(timesteps)
            padded = []
            for i, data in enumerate(grouped_data):
                if len(data) < max_t:
                    n_pad = max_t - len(data)
                    pad = np.repeat(data[-1:], n_pad, axis=0)
                    data = np.vstack([data, pad])
                    print(f"   âœ“ {sample_ids[i]}: å¡«å…… {n_pad} è¡Œ")
                padded.append(data)
            grouped_data = padded
            print(f"   âœ“ æ‰€æœ‰æ ·æœ¬å·²å¡«å……åˆ° {max_t} æ­¥")
        
        # è½¬æ¢ä¸º 3D æ•°ç»„
        weather_3d = np.array(grouped_data, dtype=np.float32)
        
        print(f"\nâœ“ è½¬æ¢ä¸º 3D æ•°ç»„:")
        print(f"   - å½¢çŠ¶: {weather_3d.shape}")
        print(f"   - ç»´åº¦: (æ ·æœ¬={len(sample_ids)}, æ—¶é—´æ­¥={weather_3d.shape[1]}, ç‰¹å¾={len(feature_cols)})")
        
        return weather_3d, sample_ids, feature_cols

    def load_weather_data_3d(self, filepath: str,
                            sample_col: str = "Env",
                            date_col: str = "Date",
                            handle_missing: str = 'drop',
                            missing_threshold: float = 0.5,
                            required_features: Optional[List[str]] = None) -> Tuple[np.ndarray, List[str], List[str]]:
        """
        åŠ è½½å¤©æ°”æ•°æ®å¹¶è¿”å› 3D æ•°ç»„
        
        Parameters:
            filepath: æ•°æ®æ–‡ä»¶è·¯å¾„
            sample_col: æ ·æœ¬IDåˆ—å
            date_col: æ—¥æœŸåˆ—å
            handle_missing: ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•
            missing_threshold: ç¼ºå¤±ç‡é˜ˆå€¼
            required_features: å¿…é¡»åŒ…å«çš„ç‰¹å¾åˆ—è¡¨ï¼ˆç”¨äºæ¨ç†æ—¶ä¿æŒä¸€è‡´æ€§ï¼‰
        
        Returns:
            tuple: (weather_3d, sample_ids, feature_names)
                   weather_3d å½¢çŠ¶: (n_samples, n_timesteps, n_features)
        """
        # å…ˆåŠ è½½ä¸º DataFrameï¼ˆä¿ç•™æ—¶é—´åºåˆ—ï¼‰
        # If required_features specified, use 'mean' instead of 'drop' to keep all columns
        if required_features:
            df_weather = self.load_weather_data(
                filepath=filepath,
                reshape_to_temporal=True,
                sample_col=sample_col,
                date_col=date_col,
                handle_missing='mean',  # Don't drop columns when we need specific features
                missing_threshold=1.0   # Don't drop any columns
            )
        else:
            df_weather = self.load_weather_data(
                filepath=filepath,
                reshape_to_temporal=True,  # ä¿ç•™æ—¶é—´åºåˆ—
                sample_col=sample_col,
                date_col=date_col,
                handle_missing=handle_missing,
                missing_threshold=missing_threshold
            )
        
        # è½¬æ¢ä¸º 3D æ•°ç»„
        weather_3d, sample_ids, feature_names = self.convert_to_3d_array(
            df_weather, 
            sample_col=sample_col, 
            date_col=date_col
        )
        
        # Reorder features to match required_features if specified
        if required_features:
            available_features = set(feature_names)
            required_set = set(required_features)
            
            if not required_set.issubset(available_features):
                missing = required_set - available_features
                print(f"âš ï¸ ç¼ºå°‘å¿…éœ€çš„ç‰¹å¾: {missing}")
                raise ValueError(f"Required features not available: {missing}")
            
            if feature_names != required_features:
                print(f"ğŸ“Œ é‡æ–°æ’åºç‰¹å¾ä»¥åŒ¹é…è®­ç»ƒé¡ºåº...")
                feature_indices = [feature_names.index(f) for f in required_features]
                weather_3d = weather_3d[:, :, feature_indices]
                feature_names = required_features.copy()
                print(f"   âœ“ ç‰¹å¾å·²é‡æ–°æ’åº: {len(feature_names)} ä¸ªç‰¹å¾")
        
        return weather_3d, sample_ids, feature_names
