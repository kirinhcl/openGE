# Sparse Transformer Configuration
# Extends base.yaml with sparsity-specific settings

model:
  name: "gxe_sparse_model"
  genetic_encoder:
    name: "weight_sparse_transformer"
    input_dim: 50000
    hidden_dim: 256
    n_layers: 2
    n_heads: 8
    sparsity_level: 0.8  # 80% sparsity

  env_encoder:
    name: "mlp"
    input_dim: 32
    hidden_dims: [128, 64]
    output_dim: 64
  
  fusion:
    name: "attention"
    hidden_dim: 256

training:
  batch_size: 32
  epochs: 150
  learning_rate: 0.0005
  weight_decay: 0.0001
  early_stopping_patience: 15

interpretability:
  enable_sparsity_analysis: true
  sparsity_analysis:
    enabled: true
    target_sparsity: 0.8
    pruning_method: "magnitude"  # or "structured"
